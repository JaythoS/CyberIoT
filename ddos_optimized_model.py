#!/usr/bin/env python3
"""
DDoS Tespit Modeli - OPTÄ°MÄ°ZE EDÄ°LMÄ°Å
Daha fazla veri ile ama optimize edilmiÅŸ ÅŸekilde
"""

import pandas as pd
import numpy as np
import os
import glob
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
import warnings
import time
import joblib
from tqdm import tqdm

warnings.filterwarnings('ignore')

class OptimizedDDoSModel:
    def __init__(self, data_path):
        self.data_path = data_path
        self.df = None
        self.model = None
        self.scaler = None

    def load_optimized_dataset(self, num_files=50):
        """Optimize edilmiÅŸ veriseti yÃ¼kle - daha fazla dosya ama hÄ±zlÄ±"""
        print(f"ğŸš€ OPTÄ°MÄ°ZE EDÄ°LMÄ°Å VERÄ°SETÄ° YÃœKLENÄ°YOR...")
        print(f"ğŸ“ {num_files} dosya kullanÄ±lacak")
        print(f"ğŸ’¾ Tahmini boyut: ~4GB")

        start_time = time.time()

        # TÃ¼m DDoS saldÄ±rÄ± tipleri
        ddos_attacks = [
            'DDoS-UDP_Flood', 'DDoS-TCP_Flood', 'DDoS-ICMP_Flood', 'DDoS-SYN_Flood',
            'DDoS-PSHACK_Flood', 'DDoS-RSTFINFlood', 'DDoS-SynonymousIP_Flood',
            'DDoS-ICMP_Fragmentation', 'DDoS-UDP_Fragmentation', 'DDoS-ACK_Fragmentation',
            'DDoS-HTTP_Flood', 'DDoS-SlowLoris'
        ]

        # CSV dosyalarÄ±nÄ± bul
        csv_files = glob.glob(os.path.join(self.data_path, "part-*.csv"))
        csv_files.sort()

        # Ä°lk num_files dosyayÄ± al
        selected_files = csv_files[:num_files]
        print(f"ğŸ“‚ SeÃ§ilen dosyalar: 1-{num_files} / {len(csv_files)}")

        data_chunks = []
        total_samples = 0
        ddos_count = 0
        benign_count = 0

        # DosyalarÄ± yÃ¼kle
        for i, file in enumerate(tqdm(selected_files, desc="Dosyalar yÃ¼kleniyor")):
            try:
                # TÃ¼m sÃ¼tunlarÄ± oku (daha kapsamlÄ± analiz iÃ§in)
                df_chunk = pd.read_csv(file, low_memory=False)

                # DDoS ve Benign verilerini filtrele
                mask = (df_chunk['label'].isin(ddos_attacks)) | (df_chunk['label'] == 'BenignTraffic')
                df_filtered = df_chunk[mask].copy()

                # Binary label oluÅŸtur
                df_filtered['target'] = df_filtered['label'].apply(
                    lambda x: 1 if x in ddos_attacks else 0
                )

                # SayÄ±larÄ± gÃ¼ncelle
                chunk_ddos = len(df_filtered[df_filtered['target'] == 1])
                chunk_benign = len(df_filtered[df_filtered['target'] == 0])
                ddos_count += chunk_ddos
                benign_count += chunk_benign
                total_samples += len(df_filtered)

                data_chunks.append(df_filtered)

                # Her 10 dosyada bir rapor ver
                if (i + 1) % 10 == 0:
                    print(f"ğŸ“Š {i+1}/{len(selected_files)} dosya yÃ¼klendi")
                    print(f"   Toplam: {total_samples:,} | DDoS: {ddos_count:,} | Benign: {benign_count:,}")

            except Exception as e:
                print(f"âŒ Hata ({file}): {e}")
                continue

        if not data_chunks:
            raise ValueError("HiÃ§ veri yÃ¼klenemedi!")

        # TÃ¼m verileri birleÅŸtir
        print("\nğŸ”— Veriler birleÅŸtiriliyor...")
        self.df = pd.concat(data_chunks, ignore_index=True)

        end_time = time.time()
        loading_time = end_time - start_time

        print(f"\nâœ… OPTÄ°MÄ°ZE EDÄ°LMÄ°Å VERÄ°SETÄ° YÃœKLENDI!")
        print(f"â±ï¸  YÃ¼kleme sÃ¼resi: {loading_time/60:.1f} dakika")
        print(f"ğŸ“Š Toplam Ã¶rnek: {len(self.df):,}")
        print(f"ğŸ”¥ DDoS saldÄ±rÄ±larÄ±: {ddos_count:,}")
        print(f"ğŸ›¡ï¸  Benign trafik: {benign_count:,}")
        print(f"ğŸ“ˆ DDoS oranÄ±: {ddos_count/len(self.df)*100:.2f}%")

        # DDoS tÃ¼rleri daÄŸÄ±lÄ±mÄ±
        print(f"\nğŸ¯ DDOS SALDIRI TÃœRLERÄ° (ilk 10):")
        ddos_types = self.df[self.df['target'] == 1]['label'].value_counts()
        for attack_type, count in ddos_types.head(10).items():
            print(f"   {attack_type:<25}: {count:,}")

        return self.df

    def prepare_optimized_data(self):
        """Optimize edilmiÅŸ veri hazÄ±rlama"""
        print(f"\nğŸ”§ OPTÄ°MÄ°ZE EDÄ°LMÄ°Å VERÄ° HAZÄ±RLAMA")

        if self.df is None:
            raise ValueError("Veri yÃ¼klenmedi!")

        # TÃ¼m sayÄ±sal Ã¶zellikleri al
        numeric_columns = self.df.select_dtypes(include=[np.number]).columns.tolist()

        # Label ve target'Ä± Ã§Ä±kar
        features = [col for col in numeric_columns if col not in ['target', 'binary_label', 'Unnamed: 0']]

        print(f"ğŸ“Š Toplam Ã¶zellik: {len(features)}")

        # Ã–zellikler ve hedef
        X = self.df[features].copy()
        y = self.df['target'].copy()

        print(f"ğŸ“Š Ã–zellik matrisi: {X.shape}")

        # Sonsuz ve NaN deÄŸerleri temizle
        print("ğŸ§¹ Veri temizleniyor...")
        X = X.replace([np.inf, -np.inf], np.nan)

        # NaN deÄŸerleri sÃ¼tun ortalamasÄ± ile doldur
        nan_columns = []
        for col in X.columns:
            if X[col].isnull().any():
                mean_val = X[col].mean()
                if not pd.isna(mean_val):  # Sadece geÃ§erli ortalamalarla doldur
                    X[col] = X[col].fillna(mean_val)
                    nan_columns.append(col)

        if nan_columns:
            print(f"   {len(nan_columns)} sÃ¼tun NaN ile dolduruldu")

        # Kalan NaN deÄŸerleri 0 ile doldur
        X = X.fillna(0)

        print(f"âœ… Veri temizlendi: {X.isnull().sum().sum()} NaN kaldÄ±")

        # %75 eÄŸitim, %25 test bÃ¶lmesi
        print("ğŸ“¦ Veri bÃ¶lÃ¼nÃ¼yor (75-25)...")
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.25, random_state=42, stratify=y
        )

        print(f"âœ… Veri hazÄ±rlandÄ±:")
        print(f"   EÄŸitim seti: {X_train.shape[0]:,} Ã¶rnek")
        print(f"   Test seti: {X_test.shape[0]:,} Ã¶rnek")
        print(f"   Ã–zellik sayÄ±sÄ±: {X_train.shape[1]}")

        # SÄ±nÄ±f daÄŸÄ±lÄ±mÄ±
        train_ddos = sum(y_train == 1)
        train_benign = sum(y_train == 0)
        test_ddos = sum(y_test == 1)
        test_benign = sum(y_test == 0)

        print(f"\nğŸ“ˆ SÄ±nÄ±f DaÄŸÄ±lÄ±mÄ±:")
        print(f"   EÄŸitim - DDoS: {train_ddos:,} ({train_ddos/len(y_train)*100:.2f}%)")
        print(f"   EÄŸitim - Benign: {train_benign:,} ({train_benign/len(y_train)*100:.2f}%)")
        print(f"   Test - DDoS: {test_ddos:,} ({test_ddos/len(y_test)*100:.2f}%)")
        print(f"   Test - Benign: {test_benign:,} ({test_benign/len(y_test)*100:.2f}%)")

        # Ã–lÃ§eklendirme
        print("âš–ï¸  Ã–zellikler Ã¶lÃ§eklendiriliyor...")
        self.scaler = StandardScaler()
        X_train_scaled = self.scaler.fit_transform(X_train)
        X_test_scaled = self.scaler.transform(X_test)

        return X_train_scaled, X_test_scaled, y_train, y_test, features

    def train_optimized_model(self, X_train, y_train):
        """Optimize edilmiÅŸ Random Forest eÄŸitimi"""
        print(f"\nğŸŒ² OPTÄ°MÄ°ZE EDÄ°LMÄ°Å RANDOM FOREST EÄÄ°TÄ°MÄ°")

        # Daha fazla Ã¶zellik iÃ§in optimize edilmiÅŸ parametreler
        self.model = RandomForestClassifier(
            n_estimators=150,          # Orta aÄŸaÃ§ sayÄ±sÄ±
            max_depth=25,              # Orta derinlik
            min_samples_split=15,      # Overfitting Ã¶nleme
            min_samples_leaf=8,        # Daha kararlÄ± yapraklar
            max_features='sqrt',       # Ã–zellik optimizasyonu
            bootstrap=True,            # Bootstrap
            oob_score=True,            # Out-of-bag skor
            class_weight='balanced',   # SÄ±nÄ±f dengesizliÄŸi
            random_state=42,
            n_jobs=-1                  # Paralel iÅŸlem
        )

        print("ğŸš€ EÄŸitim baÅŸlÄ±yor...")
        print(f"ğŸ“Š EÄŸitim verisi: {X_train.shape}")
        print(f"ğŸŒ³ AÄŸaÃ§ sayÄ±sÄ±: {self.model.n_estimators}")
        print(f"âš™ï¸  Maksimum derinlik: {self.model.max_depth}")

        start_time = time.time()
        self.model.fit(X_train, y_train)
        training_time = time.time() - start_time

        print(f"âœ… EÄŸitim tamamlandÄ±! ({training_time/60:.1f} dakika)")
        print(f"ğŸ“Š Out-of-Bag skoru: {self.model.oob_score_:.4f}")

        return self.model

    def evaluate_optimized_model(self, X_test, y_test, features):
        """Optimize edilmiÅŸ model deÄŸerlendirmesi"""
        print(f"\nğŸ“Š OPTÄ°MÄ°ZE EDÄ°LMÄ°Å MODEL DEÄERLENDÄ°RMESÄ°")

        # Tahminler
        y_pred = self.model.predict(X_test)
        y_proba = self.model.predict_proba(X_test)[:, 1]

        # Temel metrikler
        accuracy = accuracy_score(y_test, y_pred)

        print(f"ğŸ¯ Test DoÄŸruluÄŸu: {accuracy:.4f} ({accuracy*100:.2f}%)")

        # DetaylÄ± sÄ±nÄ±flandÄ±rma raporu
        print(f"\nğŸ“‹ DetaylÄ± SÄ±nÄ±flandÄ±rma Raporu:")
        print(classification_report(y_test, y_pred,
                                   target_names=['Benign', 'DDoS'],
                                   digits=4))

        # Confusion matrix
        cm = confusion_matrix(y_test, y_pred)
        tn, fp, fn, tp = cm.ravel()

        print(f"\nğŸ”¢ Confusion Matrix:")
        print(f"   True Negatives (Benign doÄŸru):    {tn:,}")
        print(f"   False Positives (Benign yanlÄ±ÅŸ):  {fp:,}")
        print(f"   False Negatives (DDoS kaÃ§Ä±rÄ±lan): {fn:,}")
        print(f"   True Positives (DDoS doÄŸru):      {tp:,}")

        # DDoS iÃ§in Ã¶zel metrikler
        detection_rate = tp / (tp + fn) if (tp + fn) > 0 else 0
        false_positive_rate = fp / (fp + tn) if (fp + tn) > 0 else 0
        precision = tp / (tp + fp) if (tp + fp) > 0 else 0

        print(f"\nğŸ¯ DDOS TESPÄ°T METRÄ°KLERÄ°:")
        print(f"   Tespit OranÄ± (Recall):    {detection_rate:.4f} ({detection_rate*100:.2f}%)")
        print(f"   YanlÄ±ÅŸ Alarm OranÄ±:       {false_positive_rate:.4f} ({false_positive_rate*100:.2f}%)")
        print(f"   Kesinlik (Precision):     {precision:.4f} ({precision*100:.2f}%)")

        # Ã–zellik Ã¶nemleri
        feature_importance = pd.DataFrame({
            'Feature': features,
            'Importance': self.model.feature_importances_
        }).sort_values('Importance', ascending=False)

        print(f"\nğŸ¯ En Ã–nemli 15 Ã–zellik:")
        for i, (idx, row) in enumerate(feature_importance.head(15).iterrows(), 1):
            print(f"   {i:2d}. {row['Feature']:<25} ({row['Importance']:.4f})")

        # CSV sonuÃ§larÄ± oluÅŸtur
        results_df = pd.DataFrame({
            'Feature': features,
            'Importance': self.model.feature_importances_
        }).sort_values('Importance', ascending=False)

        results_df.to_csv('feature_importance_optimized.csv', index=False)
        print(f"ğŸ“„ Ã–zellik Ã¶nemleri kaydedildi: feature_importance_optimized.csv")

        return {
            'accuracy': accuracy,
            'detection_rate': detection_rate,
            'false_positive_rate': false_positive_rate,
            'precision': precision,
            'confusion_matrix': cm,
            'feature_importance': feature_importance
        }

    def save_optimized_model(self, features, filename='ddos_optimized_model.pkl'):
        """Optimize edilmiÅŸ modeli kaydet"""
        print(f"\nğŸ’¾ OPTÄ°MÄ°ZE EDÄ°LMÄ°Å MODEL KAYDEDÄ°LÄ°YOR: {filename}")

        model_data = {
            'model': self.model,
            'scaler': self.scaler,
            'features': features,
            'dataset_info': {
                'total_samples': len(self.df),
                'feature_count': len(features),
                'model_type': 'Optimized Random Forest'
            }
        }

        joblib.dump(model_data, filename)
        print(f"âœ… Model baÅŸarÄ±yla kaydedildi!")
        return filename

def main():
    """Ana fonksiyon"""
    print("ğŸš€ DDOS TESPÄ°T MODELÄ° - OPTÄ°MÄ°ZE EDÄ°LMÄ°Å")
    print("Daha fazla veri ile optimize edilmiÅŸ ÅŸekilde")
    print("="*80)

    DATA_PATH = '/Users/enes/Desktop/sibers/data/external/wataiData/csv/CICIoT2023'

    try:
        # Model oluÅŸtur
        optimized_model = OptimizedDDoSModel(DATA_PATH)

        # Optimize edilmiÅŸ verisetini yÃ¼kle (50 dosya)
        df = optimized_model.load_optimized_dataset(num_files=50)

        # Veriyi hazÄ±rla
        X_train, X_test, y_train, y_test, features = optimized_model.prepare_optimized_data()

        # Model eÄŸitimi
        model = optimized_model.train_optimized_model(X_train, y_train)

        # Model deÄŸerlendirme
        results = optimized_model.evaluate_optimized_model(X_test, y_test, features)

        # Modeli kaydet
        filename = optimized_model.save_optimized_model(features)

        print(f"\nğŸ‰ OPTÄ°MÄ°ZE EDÄ°LMÄ°Å MODEL BAÅARIYLA OLUÅTURULDU!")
        print(f"ğŸ“„ Model dosyasÄ±: {filename}")
        print(f"ğŸ“Š SonuÃ§lar:")
        print(f"   DoÄŸruluk: {results['accuracy']*100:.2f}%")
        print(f"   DDoS Tespit OranÄ±: {results['detection_rate']*100:.2f}%")
        print(f"   YanlÄ±ÅŸ Alarm OranÄ±: {results['false_positive_rate']*100:.2f}%")
        print(f"   Ã–zellik SayÄ±sÄ±: {len(features)}")

    except Exception as e:
        print(f"âŒ Hata: {e}")
        raise

if __name__ == "__main__":
    main()