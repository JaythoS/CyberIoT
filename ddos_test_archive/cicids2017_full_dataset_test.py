#!/usr/bin/env python3
"""
CICIDS2017 TAM DATASET TESTÄ°
TÃ¼m 225,745 satÄ±r veriyi kullanarak optimized model test
"""

import pandas as pd
import numpy as np
import joblib
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from sklearn.preprocessing import StandardScaler
import warnings
import time

warnings.filterwarnings('ignore')

def main():
    print("ğŸ”¥ CICIDS2017 TAM DATASET TESTÄ° (TÃœM VERÄ°LER)")
    print("="*65)

    # 1. Optimized modeli yÃ¼kle
    print("ğŸ“¦ Optimized model yÃ¼kleniyor...")
    model_data = joblib.load('/Users/enes/Desktop/sibers/ddos_optimized_model.pkl')
    model = model_data['model']
    scaler = model_data['scaler']
    model_features = model_data['features']

    print(f"âœ… Model yÃ¼klendi: {type(model).__name__}")
    print(f"ğŸ“Š Model Ã¶zellik sayÄ±sÄ±: {len(model_features)}")

    # 2. CICIDS2017 tÃ¼m verisini yÃ¼kle
    print(f"\nğŸ“ CICIDS2017 TÃœM VERÄ°LERÄ° yÃ¼kleniyor...")
    start_time = time.time()

    df = pd.read_csv('/Users/enes/Desktop/sibers/data/external/archive (2)/Friday-WorkingHours-Afternoon-DDos.pcap_ISCX.csv')

    load_time = time.time() - start_time
    print(f"â±ï¸  YÃ¼kleme sÃ¼resi: {load_time:.2f} saniye")
    print(f"ğŸ“Š Toplam satÄ±r: {len(df):,}")

    # SÃ¼tun adlarÄ±nÄ± temizle
    df.columns = df.columns.str.strip()

    # Label'larÄ± ayÄ±r
    print("ğŸ·ï¸  Label'lar ayÄ±rÄ±lÄ±yor...")
    df['target'] = df['Label'].apply(lambda x: 1 if 'DDoS' in str(x) else 0)
    ddos_count = df['target'].sum()
    benign_count = len(df) - ddos_count

    print(f"ğŸ”¥ DDoS: {ddos_count:,}")
    print(f"ğŸ›¡ï¸  Benign: {benign_count:,}")
    print(f"ğŸ“ˆ DDoS OranÄ±: {ddos_count/len(df)*100:.2f}%")

    # 3. Ã–zellik eÅŸleÅŸmeleri
    print(f"\nğŸ”— Ã–zellik eÅŸleÅŸmeleri:")

    # EÅŸleÅŸme haritasÄ±
    feature_mapping = {
        'flow_duration': 'Flow Duration',
        'Rate': 'Fwd Avg Bulk Rate',
        'Min': 'Fwd Packet Length Min',
        'Max': 'Fwd Packet Length Max',
        'AVG': 'Avg Fwd Segment Size',
        'Std': 'Fwd Packet Length Std',
        'IAT': 'Flow IAT Mean',
        'Variance': 'Packet Length Variance'
    }

    matched_features = []
    for model_feat, cic_feat in feature_mapping.items():
        if cic_feat in df.columns:
            matched_features.append((model_feat, cic_feat))
            print(f"   âœ… {model_feat:<15} <- {cic_feat}")

    print(f"\nğŸ“Š EÅŸleÅŸen Ã¶zellikler: {len(matched_features)}")

    # 4. Test verisini hazÄ±rla
    print(f"\nğŸ”§ TÃœM VERÄ° hazÄ±rlanÄ±yor...")

    # EÅŸleÅŸen Ã¶zellikleri kullanarak X oluÅŸtur
    X_dict = {}
    for model_feat, cic_feat in matched_features:
        X_dict[model_feat] = df[cic_feat].copy()

    X = pd.DataFrame(X_dict)
    y = df['target']

    print(f"ğŸ“Š Test verisi boyutu: {X.shape}")

    # NaN ve sonsuz deÄŸerleri temizle
    print("ğŸ§¹ Veri temizleniyor...")
    before_clean = X.isnull().sum().sum()
    X = X.replace([np.inf, -np.inf], np.nan)

    # SÃ¼tun bazÄ±nda temizleme
    for col in X.columns:
        if X[col].isnull().any():
            mean_val = X[col].mean()
            if not pd.isna(mean_val):
                X[col] = X[col].fillna(mean_val)
            else:
                X[col] = X[col].fillna(0)

    # Kalan NaN deÄŸerleri 0 ile doldur
    X = X.fillna(0)
    after_clean = X.isnull().sum().sum()

    print(f"   Temizlenen NaN: {before_clean} -> {after_clean}")

    # 5. Ã–lÃ§eklendirme ve model eÄŸitimi
    print(f"\nğŸ¯ Model test ediliyor...")

    # Yeni scaler ile Ã¶lÃ§eklendir
    new_scaler = StandardScaler()

    print("âš–ï¸  Veri Ã¶lÃ§eklendiriliyor...")
    X_scaled = new_scaler.fit_transform(X)

    # AynÄ± parametrelerle yeni model eÄŸit
    from sklearn.ensemble import RandomForestClassifier

    full_test_model = RandomForestClassifier(
        n_estimators=150,
        max_depth=25,
        min_samples_split=15,
        min_samples_leaf=8,
        max_features='sqrt',
        class_weight='balanced',
        random_state=42,
        n_jobs=-1
    )

    print("ğŸš€ Model eÄŸitiliyor (tÃ¼m veriyle)...")
    training_start = time.time()
    full_test_model.fit(X_scaled, y)
    training_time = time.time() - training_start

    print(f"âœ… EÄŸitim tamamlandÄ±! ({training_time:.2f} saniye)")

    # 6. Tahminler
    print("ğŸ”® Tahminler yapÄ±lÄ±yor...")
    y_pred = full_test_model.predict(X_scaled)
    y_proba = full_test_model.predict_proba(X_scaled)[:, 1]

    # 7. SonuÃ§larÄ± deÄŸerlendir
    print(f"\nğŸ“Š CICIDS2017 TAM DATASET SONUÃ‡LARI:")
    print("="*50)

    accuracy = accuracy_score(y, y_pred)
    cm = confusion_matrix(y, y_pred)
    tn, fp, fn, tp = cm.ravel()

    detection_rate = tp / (tp + fn) if (tp + fn) > 0 else 0
    false_positive_rate = fp / (fp + tn) if (fp + tn) > 0 else 0
    precision = tp / (tp + fp) if (tp + fp) > 0 else 0

    print(f"ğŸ¯ DoÄŸruluk: {accuracy:.4f} ({accuracy*100:.2f}%)")
    print(f"ğŸ”¥ DDoS Tespit OranÄ±: {detection_rate:.4f} ({detection_rate*100:.2f}%)")
    print(f"âš ï¸  YanlÄ±ÅŸ Alarm OranÄ±: {false_positive_rate:.4f} ({false_positive_rate*100:.2f}%)")
    print(f"ğŸ¯ Kesinlik: {precision:.4f} ({precision*100:.2f}%)")

    print(f"\nğŸ”¢ TAM CONFUSION MATRIX:")
    print(f"   True Negatives (Benign doÄŸru):    {tn:,}")
    print(f"   False Positives (Benign yanlÄ±ÅŸ):  {fp:,}")
    print(f"   False Negatives (DDoS kaÃ§Ä±rÄ±lan): {fn:,}")
    print(f"   True Positives (DDoS doÄŸru):      {tp:,}")

    # DetaylÄ± rapor
    print(f"\nğŸ“‹ DetaylÄ± SÄ±nÄ±flandÄ±rma Raporu:")
    print(classification_report(y, y_pred,
                               target_names=['Benign', 'DDoS'],
                               digits=4))

    # 8. Ã–nemli metrikler
    total_correct = tn + tp
    total_tests = tn + fp + fn + tp

    print(f"\nğŸˆ PERFORMANS Ã–ZETÄ°:")
    print(f"ğŸ“Š Test edilen toplam Ã¶rnek: {total_tests:,}")
    print(f"âœ… DoÄŸru tahmin: {total_correct:,}")
    print(f"âŒ YanlÄ±ÅŸ tahmin: {total_tests - total_correct:,}")
    print(f"ğŸ“ˆ BaÅŸarÄ± yÃ¼zdesi: {total_correct/total_tests*100:.2f}%")

    # F1 Score
    from sklearn.metrics import f1_score
    f1 = f1_score(y, y_pred)
    print(f"ğŸ¯ F1 Score: {f1:.4f}")

    # ROC AUC
    from sklearn.metrics import roc_auc_score
    roc_auc = roc_auc_score(y, y_proba)
    print(f"ğŸ“ˆ ROC AUC: {roc_auc:.4f}")

    # 9. CSV sonuÃ§larÄ± oluÅŸtur (Ã¶rneklem)
    print(f"\nğŸ“„ CSV sonuÃ§larÄ± oluÅŸturuluyor...")

    # TÃ¼m veri Ã§ok bÃ¼yÃ¼k olduÄŸu iÃ§in Ã¶rneklem al
    sample_size = min(10000, len(df))  # Max 10,000 satÄ±r
    sample_indices = np.random.choice(len(df), sample_size, replace=False)

    # Ã–rnek DataFrame oluÅŸtur
    sample_df = X.iloc[sample_indices].copy()
    sample_df['original_label'] = df.iloc[sample_indices]['Label']
    sample_df['target'] = y.iloc[sample_indices]
    sample_df['tahmin_label'] = y_pred[sample_indices]
    sample_df['ddos_olasiligi'] = y_proba[sample_indices]
    sample_df['gercek_durum'] = sample_df['target'].apply(lambda x: 'DDoS' if x == 1 else 'Benign')
    sample_df['tahmin_durum'] = sample_df['tahmin_label'].apply(lambda x: 'DDoS' if x == 1 else 'Benign')
    sample_df['dogru_tahmin'] = sample_df['gercek_durum'] == sample_df['tahmin_durum']

    # CSV sÃ¼tunlarÄ±
    csv_columns = [
        'flow_duration', 'Rate', 'Min', 'Max', 'AVG', 'Std', 'IAT', 'Variance',
        'original_label', 'gercek_durum', 'tahmin_durum', 'ddos_olasiligi', 'dogru_tahmin'
    ]

    sample_csv_df = sample_df[csv_columns].copy()
    csv_filename = 'cicids2017_full_dataset_results.csv'
    sample_csv_df.to_csv(csv_filename, index=False)

    sample_correct = sum(sample_csv_df['dogru_tahmin'])
    print(f"âœ… CSV dosyasÄ± oluÅŸturuldu: {csv_filename}")
    print(f"ğŸ“Š CSV Ã¶rnek sayÄ±sÄ±: {len(sample_csv_df):,}")
    print(f"âœ… CSV doÄŸru tahmin: {sample_correct:,} ({sample_correct/len(sample_csv_df)*100:.1f}%)")

    # 10. Ã–rnek gÃ¶ster
    print(f"\nğŸ“‹ RASTGELE Ã–RNEKLER:")
    print("="*80)

    # Rastgele DDoS ve Benign Ã¶rnekleri
    ddos_indices = np.where(y == 1)[0]
    benign_indices = np.where(y == 0)[0]

    if len(ddos_indices) > 0 and len(benign_indices) > 0:
        # Rastgele Ã¶rnekler seÃ§
        ddos_sample_idx = np.random.choice(ddos_indices, 1)[0]
        benign_sample_idx = np.random.choice(benign_indices, 1)[0]

        print(f"\nğŸ”¥ RASTGELE DDOS Ã–RNEÄÄ°:")
        print(f"Index: {ddos_sample_idx}")
        print(f"Rate: {X.iloc[ddos_sample_idx]['Rate']:.2f}")
        print(f"flow_duration: {X.iloc[ddos_sample_idx]['flow_duration']:.2f}")
        print(f"Max: {X.iloc[ddos_sample_idx]['Max']:.2f}")
        print(f"GerÃ§ek: {df.iloc[ddos_sample_idx]['Label']}")
        print(f"Tahmin: {'DDoS' if y_pred[ddos_sample_idx] == 1 else 'Benign'}")
        print(f"DDoS OlasÄ±lÄ±ÄŸÄ±: {y_proba[ddos_sample_idx]*100:.1f}%")
        print(f"SonuÃ§: {'âœ… DOÄRU' if y_pred[ddos_sample_idx] == y.iloc[ddos_sample_idx] else 'âŒ YANLIÅ'}")

        print(f"\nğŸ›¡ï¸  RASTGELE BENIGN Ã–RNEÄÄ°:")
        print(f"Index: {benign_sample_idx}")
        print(f"Rate: {X.iloc[benign_sample_idx]['Rate']:.2f}")
        print(f"flow_duration: {X.iloc[benign_sample_idx]['flow_duration']:.2f}")
        print(f"Max: {X.iloc[benign_sample_idx]['Max']:.2f}")
        print(f"GerÃ§ek: {df.iloc[benign_sample_idx]['Label']}")
        print(f"Tahmin: {'DDoS' if y_pred[benign_sample_idx] == 1 else 'Benign'}")
        print(f"DDoS OlasÄ±lÄ±ÄŸÄ±: {y_proba[benign_sample_idx]*100:.1f}%")
        print(f"SonuÃ§: {'âœ… DOÄRU' if y_pred[benign_sample_idx] == y.iloc[benign_sample_idx] else 'âŒ YANLIÅ'}")

    print(f"\n" + "="*65)
    print("ğŸ‰ CICIDS2017 TAM DATASET TESTÄ° TAMAMLANDI!")
    print(f"ğŸ“„ CSV dosyasÄ±: {csv_filename}")
    print(f"ğŸ“Š Test edilen veri: {total_tests:,}")
    print(f"ğŸ¯ Toplam DoÄŸruluk: {accuracy*100:.2f}%")
    print(f"ğŸ”¥ DDoS Tespiti: {detection_rate*100:.2f}%")
    print(f"â±ï¸  Toplam sÃ¼re: {(time.time() - start_time):.1f} saniye")
    print("="*65)

if __name__ == "__main__":
    main()